{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import itertools\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.cross_validation\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "path = home + \"/Dropbox/python/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { \n",
       "\n",
       "  width:100% !important;\n",
       "  background-color: #050505 !important;\n",
       " }\n",
       "/*body {background-color: #1b202b;}*/\n",
       "\n",
       "\n",
       "/* GLOBALS */\n",
       "body {background-color: #050505; color: #777;}\n",
       "a {color: #8fa1b3;}\n",
       "\n",
       "/* INTRO PAGE */\n",
       ".toolbar_info, .list_container {color: #050505;}\n",
       "\n",
       "/* NOTEBOOK */\n",
       "\n",
       "/* comment out this line to bring the toolbar back */\n",
       "/*div#maintoolbar, div#header {display: none !important;}\n",
       "*/\n",
       "\n",
       "div.header {background-color: #050505;}\n",
       "div.lower-header-bar {background-color: #1b202b;}\n",
       "\n",
       "div#maintoolbar {background-color: #050505;}\n",
       "div#maintoolbar.navbar {background-color: #050505;}\n",
       "\n",
       "div#notebook {border-top: none; background-color: #050505;}\n",
       "div.navbar-collapse {background-color: #050505;}\n",
       "div#maintoolbar-container {display: none !important; background-color: 050505;}\n",
       "/*div#header-container {display: none !important;}\n",
       "*/\n",
       "\n",
       "div.input_prompt {color: #050505;}\n",
       "div.output_prompt {color: #050505;}\n",
       "div.input_area {\n",
       "  border-radius: 0px;\n",
       "  border: 1px solid #4f5b66;\n",
       "}\n",
       "div.output_area pre {font-weight: normal; color: #c0c5ce;}\n",
       "div.output_subarea {font-weight: normal; color: #c0c5ce;}\n",
       "\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "  border: 1px  #c0c5ce solid;\n",
       "  color: #c0c5ce;\n",
       "}\n",
       "div.output_html { font-family: sans-serif; }\n",
       "table.dataframe tr {border: 1px #c0c5ce;}\n",
       "\n",
       "div.cell.selected {border-radius: 0px;}\n",
       "div.cell.edit_mode {border-radius: 0px; border: thin solid #b48ead;}\n",
       "div.text_cell_render, div.output_html {color: #c0c5ce;}\n",
       "\n",
       "span.ansiblack {color: #343d46;}\n",
       "span.ansiblue {color: #96b5b4;}\n",
       "span.ansigray {color: #a7adba;}\n",
       "span.ansigreen {color: #a3be8c;}\n",
       "span.ansipurple {color: #b48ead;}\n",
       "span.ansired {color: #bf616a;}\n",
       "span.ansiyellow {color: #ebcb8b;}\n",
       "\n",
       "div.output_stderr {background-color: #050505;}\n",
       "div.output_stderr pre {color: #dfe1e8;}\n",
       "\n",
       ".cm-s-ipython.CodeMirror {background: #050505; color: #dfe1e8;}\n",
       ".cm-s-ipython div.CodeMirror-selected {background: #343d46 !important;}\n",
       ".cm-s-ipython .CodeMirror-gutters {background: #2b303b; border-right: 0px;}\n",
       ".cm-s-ipython .CodeMirror-linenumber {color: #65737e;}\n",
       ".cm-s-ipython .CodeMirror-cursor {border-left: 1px solid #a7adba !important;}\n",
       "\n",
       ".cm-s-ipython span.cm-comment {color: #ab7967;}\n",
       ".cm-s-ipython span.cm-atom {color: #b48ead;}\n",
       ".cm-s-ipython span.cm-number {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython span.cm-property, .cm-s-ipython span.cm-attribute {color: #a3be8c;}\n",
       ".cm-s-ipython span.cm-keyword {color: #bf616a;}\n",
       ".cm-s-ipython span.cm-string {color: #ebcb8b;}\n",
       ".cm-s-ipython span.cm-operator {color: #ab7967;}\n",
       ".cm-s-ipython span.cm-builtin {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython span.cm-variable {color: #a3be8c;}\n",
       ".cm-s-ipython span.cm-variable-2 {color: #8fa1b3;}\n",
       ".cm-s-ipython span.cm-def {color: #d08770;}\n",
       ".cm-s-ipython span.cm-error {background: #bf616a; color: #a7adba;}\n",
       ".cm-s-ipython span.cm-bracket {color: #c0c5ce;}\n",
       ".cm-s-ipython span.cm-tag {color: #bf616a;}\n",
       ".cm-s-ipython span.cm-link {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython .CodeMirror-matchingbracket { text-decoration: underline; color: #dfe1e8 !important;}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"/Users/jnoxon/Dropbox/python/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "path = home + \"/Dropbox/python/\"\n",
    "iris = pd.read_csv(\"iris.data\", header=None)\n",
    "y = iris[4]\n",
    "iris = iris.drop([4], 1)\n",
    "# x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(iris, y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "Y = df['quality'].values\n",
    "df = df.drop('quality',1)\n",
    "# x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(df, Y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('breast_cancer.csv')\n",
    "#http://www.is.umk.pl/projects/datasets.html\n",
    "y = np.array(cancer['2.1'])\n",
    "x = np.array(cancer.drop(['2.1', '1.3'], 1))\n",
    "# x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(x, y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "votes = pd.read_csv('votes.csv')\n",
    "votes = votes.replace('?', float('NaN'))\n",
    "votes = votes.dropna()\n",
    "votes = votes.reset_index(drop=True)\n",
    "votes['0'] = votes.replace('D', 1)\n",
    "votes['0'] = votes.replace('R', 0)\n",
    "y = np.array(votes['0'])\n",
    "votes = votes.drop('0', axis = 1)\n",
    "votes = votes.replace('y', 1)\n",
    "votes = votes.replace('n', 0)\n",
    "x = np.array(votes)\n",
    "# x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(np.array(x), np.array(y), test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(hyperparameters='individual'):\n",
    "    iris = pd.read_csv(\"iris.data\", header=None)\n",
    "    y = iris[4]\n",
    "    iris = iris.drop([4], 1)\n",
    "    print \"Iris:\"\n",
    "    \n",
    "    problem = classifier(iris,y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "    Y = df['quality'].values\n",
    "    df = df.drop('quality',1)\n",
    "    print\"\\nWine Quality:\"\n",
    "    \n",
    "    problem = classifier(df, Y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    cancer = pd.read_csv('breast_cancer.csv')\n",
    "    #http://www.is.umk.pl/projects/datasets.html\n",
    "    y = np.array(cancer['2.1'])\n",
    "    x = np.array(cancer.drop(['2.1', '1.3'], 1))\n",
    "    print \"\\nBreast Cancer:\"\n",
    "    \n",
    "    problem = classifier(x, y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    votes = pd.read_csv('votes.csv')\n",
    "    votes = votes.replace('?', float('NaN'))\n",
    "    votes = votes.dropna()\n",
    "    votes = votes.reset_index(drop=True)\n",
    "    votes['0'] = votes.replace('D', 1)\n",
    "    votes['0'] = votes.replace('R', 0)\n",
    "    y = np.array(votes['0'])\n",
    "    votes = votes.drop('0', axis = 1)\n",
    "    votes = votes.replace('y', 1)\n",
    "    votes = votes.replace('n', 0)\n",
    "    x = np.array(votes)\n",
    "    print \"\\nVotes:\"\n",
    "    \n",
    "    problem = classifier(x, y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import itertools\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.cross_validation\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "class apm():\n",
    "    def __init__(self, x, y, task, test=None, hyperparameters='individual', debug=False):\n",
    "        self.time = time.time()\n",
    "        self.task = task\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.x = np.array(x)\n",
    "        self.y = y\n",
    "        if self.task == 'classification':\n",
    "            if type(self.y[0]) != int:\n",
    "                self.y, self.mapping = self.categorize(self.y)\n",
    "            self.y = np.array(self.y, dtype = int)\n",
    "        elif self.task == 'regression':\n",
    "            self.y = np.array(self.y, dtype = float)\n",
    "        if test is None:\n",
    "            self.test = None\n",
    "        else:\n",
    "            self.test = self.feature_transform(np.array(test))\n",
    "            \n",
    "        self.x = self.feature_transform(self.x)\n",
    "        \n",
    "        \n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = sklearn.cross_validation.train_test_split\\\n",
    "            (self.x, self.y, test_size = .20, random_state=42)\n",
    "        self.x_train1 = self.x_train[:len(self.x_train)/2]\n",
    "        self.x_train2 = self.x_train[len(self.x_train)/2:]\n",
    "        self.y_train1 = self.y_train[:len(self.x_train)/2]\n",
    "        self.y_train2 = self.y_train[len(self.x_train)/2:]\n",
    "        self.stacking_train = self.x_train2\n",
    "        self.stacking_test = self.x_test\n",
    "        \n",
    "        self.x1 = self.x[:len(self.x)/2]\n",
    "        self.x2 = self.x[len(self.x)/2:]\n",
    "        self.y1 = self.y[:len(self.x)/2]\n",
    "        self.y2 = self.y[len(self.x)/2:]\n",
    "        self.test_stacking_train = self.x2\n",
    "        self.test_stacking_test = self.test\n",
    "\n",
    "        self.debug = False\n",
    "        self.stacking = False\n",
    "        self.testing = False\n",
    "        self.optimal_iterator = None\n",
    "        \n",
    "        if self.task == 'classification':\n",
    "            if self.debug:\n",
    "                self.algorithms = [self.logistic_regression]\n",
    "                self.algorithm_names = ['logistic_predictions']\n",
    "            else:\n",
    "                self.algorithms = [self.xgboost, self.gradient_booster, self.random_forest,\\\n",
    "                      self.extremely_random_forest, self.logistic_regression, self.lda, self.qda]\n",
    "                self.algorithm_names = ['xgboost_predictions','booster_predictions', 'forest_predictions', 'erf_predictions',\\\n",
    "                          'logistic_predictions', 'lda_predictions', 'qda_predictions', \\\n",
    "                           'xgboost_on_stack', 'booster_on_stack','forest_on_stack', \\\n",
    "                            'erf_on_stack', 'logistic_on_stack', 'lda_on_stack', 'qda_on_stack']\n",
    "        elif self.task == 'regression':\n",
    "            if self.debug:\n",
    "                self.algorithms = [self.linear_regression, self.ridge_regression, self.lasso_regression,\\\n",
    "                                   self.gradient_booster, self.random_forest, self.extremely_random_forest]\n",
    "                self.algorithm_names = ['linear_predictions', 'ridge_predictions', 'lasso_predictions',\\\n",
    "                                        'booster_predictions', 'forest_predictions', 'erf_predictions', 'linear_on_stack',\\\n",
    "                                        'ridge_on_stack', 'lasso_on_stack', 'booster_on_stack', 'forest_on_stack', 'erf_on_stack']\n",
    "            else:\n",
    "                self.algorithms = [self.linear_regression, self.ridge_regression, self.lasso_regression,\\\n",
    "                                   self.gradient_booster, self.random_forest, self.extremely_random_forest]\n",
    "                self.algorithm_names = ['linear_predictions', 'ridge_predictions', 'lasso_predictions',\\\n",
    "                                        'booster_predictions', 'forest_predictions', 'erf_predictions', 'linear_on_stack',\\\n",
    "                                        'ridge_on_stack', 'lasso_on_stack', 'booster_on_stack', 'forest_on_stack', 'erf_on_stack']\n",
    "                \n",
    "        self.xgb_max_depth = None\n",
    "        self.gb_n_estimators = None\n",
    "        self.gb_max_depth = None\n",
    "        self.rf_n_estimators = 400\n",
    "        self.rf_max_features = \"auto\"\n",
    "        self.erf_n_estimators = None\n",
    "        self.erf_max_features = None\n",
    "        self.log_reg_penalty = None\n",
    "        self.log_reg_C = None\n",
    "        self.qda_reg_param = None\n",
    "        self.svc_C = None\n",
    "        self.svc_kernel = None\n",
    "        \n",
    "        self.training_df = None\n",
    "        self.testing_df = None        \n",
    "        \n",
    "    \n",
    "    def accuracy(self, predictions):\n",
    "        if len(predictions) == len(self.y_test):\n",
    "            y_test = self.y_test\n",
    "        elif len(predictions) == len(self.y_train2):\n",
    "            y_test = self.y_train2\n",
    "        elif len(predictions) == len(self.y2):\n",
    "            y_test = self.y2\n",
    "        else:\n",
    "            print \"Accuracy called with incorrectly sized predictions\"\n",
    "        if self.task == 'classification':\n",
    "            count = 0\n",
    "            for i in xrange(len(predictions)):\n",
    "                if predictions[i] == y_test[i]:\n",
    "                    count += 1\n",
    "            accuracy = count/float(len(predictions))\n",
    "        elif self.task == 'regression':\n",
    "            error = [np.abs(predictions[i] - y_test[i]) for i in xrange(len(predictions))]\n",
    "            accuracy = np.median(error)\n",
    "        return accuracy\n",
    "    \n",
    "    def categorize(self, labels):\n",
    "        mapping = {}\n",
    "        labels = pd.Series(labels)\n",
    "        for i,v in enumerate(list(set(labels))):\n",
    "            labels = labels.replace(v, i)\n",
    "            mapping[i] = v\n",
    "        return np.array(labels), mapping  \n",
    "    \n",
    "    def stacker(self):\n",
    "        self.stacking = True\n",
    "        self.train_stacker()\n",
    "        self.stacking = False\n",
    "        return self.test_stacker()\n",
    "    \n",
    "    def time_elapsed(self):\n",
    "        seconds = time.time()-self.time\n",
    "        hours = int(seconds/3600)\n",
    "        seconds = seconds-hours*3600\n",
    "        minutes = int(seconds/60)\n",
    "        seconds = seconds - minutes*60\n",
    "        print \"Time Elapsed = %dh %dm %ds\" %(hours, minutes, seconds)\n",
    "        \n",
    "    def mean(self, l):\n",
    "        means = []\n",
    "        for i in xrange(len(l[0])):\n",
    "            s = 0\n",
    "            for j in l:\n",
    "                s += j[i]\n",
    "            means.append(float(s)/len(l))\n",
    "        return means\n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        data = pd.DataFrame(x)\n",
    "        for i in data:\n",
    "            data[i] = data[i] - data[i].mean()\n",
    "#             data[i] = data[i]/float(data[i].std())\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def feature_transform(self, x):\n",
    "#         x = sklearn.preprocessing.scale(x)\n",
    "#         scale = sklearn.preprocessing.MinMaxScaler()\n",
    "        return x #np.column_stack((x, np.exp(x), np.sqrt(scale.fit_transform(x))))\n",
    "        \n",
    "    \n",
    "    def train_stacker(self):\n",
    "        for model in self.algorithms:\n",
    "            generate_train, generate_test = model()\n",
    "            if self.testing == False:\n",
    "                self.stacking_train = np.column_stack((self.stacking_train, generate_train))\n",
    "                self.stacking_test = np.column_stack((self.stacking_test, generate_test))\n",
    "            else:\n",
    "                self.test_stacking_train = np.column_stack((self.test_stacking_train, generate_train))\n",
    "                self.test_stacking_test = np.column_stack((self.test_stacking_test, generate_test))\n",
    "        return\n",
    "        \n",
    "    def test_stacker(self):\n",
    "        if self.testing == False:\n",
    "            return [model(self.stacking_train, self.y_train2, self.stacking_test, self.y_test) \\\n",
    "                for model in self.algorithms]\n",
    "        else:\n",
    "            return [model(self.test_stacking_train, self.y2, self.test_stacking_test, None)\\\n",
    "                for model in self.algorithms]\n",
    "            \n",
    "    def initialize_data(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        if self.testing == False:\n",
    "            if self.stacking == False:\n",
    "                return self.x_train, self.y_train, self.x_test, self.y_test\n",
    "            else:\n",
    "                if x_train == None:\n",
    "                    x_train = self.x_train1\n",
    "                if y_train == None:\n",
    "                    y_train = self.y_train1\n",
    "                if x_test == None:\n",
    "                    x_test = self.x_train2\n",
    "                if y_test == None:\n",
    "                    y_test = self.y_train2\n",
    "                return x_train, y_train, x_test, y_test\n",
    "        else:\n",
    "            if self.stacking == False:\n",
    "                return self.x, self.y, self.test, None\n",
    "            else:\n",
    "                if x_train == None:\n",
    "                    x_train = self.x1\n",
    "                if y_train == None:\n",
    "                    y_train = self.y1\n",
    "                if x_test == None:\n",
    "                    x_test = self.x2\n",
    "                if y_test == None:\n",
    "                    y_test = self.y2\n",
    "                return x_train, y_train, x_test, y_test\n",
    "    \n",
    "    def stacking_test_data(self):\n",
    "        if self.testing == False:\n",
    "            return self.x_test\n",
    "        else:\n",
    "            return self.test\n",
    "        \n",
    "    def _linear_regression(self, x_train, y_train, x_test):\n",
    "        lr = linear_model.LinearRegression()\n",
    "        lr = lr.fit(x_train, y_train)\n",
    "        return lr.predict(x_test)\n",
    "    \n",
    "    def linear_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        linear_predictions = self._linear_regression(x_train, y_train, x_test)\n",
    "        if self.stacking == True:\n",
    "            return linear_predictions, self._linear_regression(x_train, y_train, self.stacking_test_data())\n",
    "        return linear_predictions\n",
    "    \n",
    "    def _ridge_regression(self, x_train, y_train, x_test):\n",
    "        lr = linear_model.Ridge()\n",
    "        lr = lr.fit(x_train, y_train)\n",
    "        return lr.predict(x_test)\n",
    "    \n",
    "    def ridge_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        linear_predictions = self._ridge_regression(x_train, y_train, x_test)\n",
    "        if self.stacking == True:\n",
    "            return linear_predictions, self._ridge_regression(x_train, y_train, self.stacking_test_data())\n",
    "        return linear_predictions\n",
    "    \n",
    "    def _lasso_regression(self, x_train, y_train, x_test):\n",
    "        lr = linear_model.Lasso()\n",
    "        lr = lr.fit(x_train, y_train)\n",
    "        return lr.predict(x_test)\n",
    "    \n",
    "    def lasso_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        linear_predictions = self._lasso_regression(x_train, y_train, x_test)\n",
    "        self.linear_preds = linear_predictions\n",
    "        if self.stacking == True:\n",
    "            return linear_predictions, self._lasso_regression(x_train, y_train, self.stacking_test_data())\n",
    "        return linear_predictions\n",
    "        \n",
    "    def xgboost(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "        if self.task == 'classification':\n",
    "            param = {'bst:max_depth':50, 'bst:eta':1, 'silent':1,'objective':'multi:softmax'}\n",
    "#         elif self.task == 'regression':\n",
    "#             param = {'bst:max_depth':50, 'bst:eta':1, 'silent':1,'objective':'reg:linear'}\n",
    "        param['num_class'] = x_train.shape[1]\n",
    "#         evallist = [(dtest,'eval'), (dtrain,'train')]\n",
    "        num_round = 10\n",
    "        bst = xgb.train(param, dtrain, num_round, verbose_eval=False)\n",
    "        preds = bst.predict(dtest)\n",
    "        self.xgb_predictions = preds\n",
    "#         for i in xrange(len(labels)):\n",
    "#             print labels[i], self.y_test[i]\n",
    "        if self.stacking == True:\n",
    "            return preds, bst.predict(xgb.DMatrix(self.stacking_test_data()))\n",
    "        return preds\n",
    "    \n",
    "    def gb(self, x_train=None, y_train=None, x_test=None, n_estimators=400, max_depth=2):\n",
    "        if self.task == 'classification':\n",
    "            booster = ensemble.GradientBoostingClassifier(n_estimators = n_estimators, max_depth = max_depth)\n",
    "        elif self.task == 'regression':\n",
    "            booster = ensemble.GradientBoostingRegressor(n_estimators = n_estimators, max_depth = max_depth)  \n",
    "        booster = booster.fit(x_train, y_train)\n",
    "        return booster.predict(x_test)\n",
    "    \n",
    "    def gradient_booster(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            booster_predictions = self.gb(x_train, y_train, x_test)\n",
    "            if self.stacking == True:\n",
    "                return booster_predictions, self.gb(x_train, y_train, self.stacking_test_data())\n",
    "            return booster_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.gb_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1300, 300):\n",
    "                    booster_predictions = self.gb(x_train, y_train, x_test, n_estimators=n_estimators)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        self.gb_n_estimators = n_estimators\n",
    "            if self.gb_max_depth == None:\n",
    "                max_accuracy = -1\n",
    "                for max_depth in xrange(2,15,3):\n",
    "                    booster_predictions = self.gb(x_train, y_train, x_test, max_depth = max_depth)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        self.gb_max_depth = max_depth\n",
    "                print \"Optimal Booster Hyperparams: n_estimators = %d, max_depth = %d\" \\\n",
    "                        %(self.gb_n_estimators, self.gb_max_depth)\n",
    "            booster_predictions = self.gb(x_train, y_train, x_test, n_estimators=self.gb_n_estimators,\\\n",
    "                                          max_depth=self.gb_max_depth)\n",
    "            if self.stacking == True:\n",
    "                return booster_predictions, self.gb(x_train, y_train, self.stacking_test_data())\n",
    "            return booster_predictions\n",
    "        elif self.hyperparameters == 'grid':\n",
    "            max_accuracy = -1\n",
    "            for n_estimators in xrange(100, 1000, 300):\n",
    "                for max_depth in xrange(2,15,3):\n",
    "                    booster_predictions = self.gb(x_train, y_train, x_test, n_estimators=n_estimators, max_depth = max_depth)\n",
    "                    print str(self.accuracy(booster_predictions)) + ' ' + str(n_estimators) + ' ' + str(max_depth)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        max_predictions = booster_predictions\n",
    "                        optimal_parameters = [n_estimators, max_depth]\n",
    "            print(\"Gradient booster: \", optimal_parameters)\n",
    "            return max_predictions\n",
    "                \n",
    "    def rf(self, x_train, y_train, x_test, n_estimators=400, max_features=\"auto\"):\n",
    "        if self.task == 'classification':\n",
    "            forest = ensemble.RandomForestClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "        elif self.task == 'regression':\n",
    "            forest = ensemble.RandomForestRegressor(n_estimators=n_estimators, max_features=max_features)\n",
    "        forest = forest.fit(x_train, y_train)\n",
    "        return forest.predict(x_test)            \n",
    "        \n",
    "    \n",
    "    def random_forest(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            if self.stacking == True:\n",
    "                return self.rf(x_train, y_train, x_test), self.rf(x_train, y_train, self.stacking_test_data())\n",
    "            return self.rf(x_train, y_train, x_test)\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.rf_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1400, 400):\n",
    "                    forest_predictions = self.rf(x_train, y_train, x_test, n_estimators=n_estimators)\n",
    "                    if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(forest_predictions)\n",
    "                        self.rf_n_estimators = n_estimators\n",
    "            if self.rf_max_features == None:\n",
    "                feat = x_train.shape[1]\n",
    "                max_accuracy = -1\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.05*feat)+1):\n",
    "                    print max_features, feat\n",
    "                    if max_features == 0:\n",
    "                        max_features = 1\n",
    "                    forest_predictions = self.rf(x_train, y_train, x_test, max_features=max_features)\n",
    "                    if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(forest_predictions)\n",
    "                        self.rf_max_features = max_features\n",
    "                print \"Optimal Forest Hyperparams: n_estimators = %d, max_features = %d\" \\\n",
    "                        %(self.rf_n_estimators, self.rf_max_features)\n",
    "            forest_predictions = self.rf(x_train, y_train, x_test,\\\n",
    "                 n_estimators=self.rf_n_estimators, max_features = self.rf_max_features)\n",
    "            if self.stacking == True:\n",
    "                return forest_predictions, self.rf(x_train, y_train, self.stacking_test_data(),\\\n",
    "                                                   self.rf_n_estimators, self.rf_max_features)\n",
    "            return forest_predictions \n",
    "        elif self.hyperparameters == 'grid':\n",
    "            feat = x_train.shape[1]\n",
    "            max_accuracy = -1\n",
    "            for n_estimators in xrange(100, 1000, 200):\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.03*feat)+1):\n",
    "                    if max_features == 0:\n",
    "                        max_features = 1\n",
    "                    for criterion in ['gini', 'entropy']:\n",
    "                        forest_predictions = self.rf(x_train, y_train, x_test, n_estimators = \\\n",
    "                                 n_estimators, criterion = criterion, max_features = max_features)\n",
    "                        print self.accuracy(forest_predictions)\n",
    "                        if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                            max_accuracy = self.accuracy(forest_predictions)\n",
    "                            max_predictions = forest_predictions\n",
    "                            optimal_parameters = [n_estimators, max_features, criterion]\n",
    "            print[\"Random forest: \", optimal_parameters]\n",
    "            return max_predictions\n",
    "\n",
    "    def erf(self, x_train, y_train, x_test, n_estimators=400, max_features=\"auto\"):\n",
    "        if self.task == 'classification':\n",
    "            forest = ensemble.ExtraTreesClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "        elif self.task == 'regression':\n",
    "            forest = ensemble.ExtraTreesRegressor(n_estimators=n_estimators, max_features=max_features)\n",
    "        forest = forest.fit(x_train, y_train)\n",
    "        return forest.predict(x_test)         \n",
    "    \n",
    "    def extremely_random_forest(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            if self.stacking == True:\n",
    "                return self.erf(x_train, y_train, x_test), self.erf(x_train, y_train, self.stacking_test_data())\n",
    "            return self.erf(x_train, y_train, x_test)\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.erf_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1300, 400):\n",
    "                    erf_predictions = self.erf(x_train, y_train, x_test, n_estimators = n_estimators)\n",
    "                    if self.accuracy(erf_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(erf_predictions)\n",
    "                        self.erf_n_estimators = n_estimators\n",
    "            if self.erf_max_features == None:\n",
    "                feat = x_train.shape[1]\n",
    "                max_accuracy = -1\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.05*feat)+1):\n",
    "                    erf_predictions = self.erf(x_train, y_train, x_test, max_features=max_features)\n",
    "                    if self.accuracy(erf_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(erf_predictions)\n",
    "                        self.erf_max_features = max_features\n",
    "                print \"Optimal erf Hyperparams: n_estimators = %d, max_features = %d\" \\\n",
    "                        %(self.erf_n_estimators, self.erf_max_features)\n",
    "            erf_predictions = self.erf(x_train, y_train, x_test,\\\n",
    "                 n_estimators=self.erf_n_estimators, max_features = self.erf_max_features)\n",
    "            if self.stacking == True:\n",
    "                return erf_predictions, self.erf(x_train, y_train, self.stacking_test_data(),\\\n",
    "                         n_estimators=self.erf_n_estimators, max_features = self.erf_max_features)\n",
    "            return erf_predictions\n",
    "    \n",
    "    def logistic_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            logistic = linear_model.LogisticRegression(penalty=\"l2\", C=1)#class_weight='auto'\n",
    "            logistic.fit(x_train, y_train)\n",
    "            logistic_predictions = logistic.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return logistic_predictions, logistic.predict(self.stacking_test_data())\n",
    "            return logistic_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.log_reg_penalty == None:\n",
    "                for penalty in ['l1', 'l2']:\n",
    "                    if penalty == 'l1':\n",
    "                        logistic = linear_model.LogisticRegression(penalty=\"l1\")#class_weight='auto'\n",
    "                        logistic.fit(x_train, y_train)\n",
    "                        logistic_predictions = logistic.predict(x_test)\n",
    "                        l1_accuracy = self.accuracy(logistic_predictions)\n",
    "                        self.log_reg_penalty = 'l1'\n",
    "                    else:\n",
    "                        logistic = linear_model.LogisticRegression(penalty=\"l2\")#class_weight='auto'\n",
    "                        logistic.fit(x_train, y_train)\n",
    "                        logistic_predictions = logistic.predict(x_test)\n",
    "                        if self.accuracy(logistic_predictions) > l1_accuracy:\n",
    "                            self.log_reg_penalty = 'l2'\n",
    "            if self.log_reg_C == None:\n",
    "                max_accuracy = -1\n",
    "                for C in xrange(1,104,4):\n",
    "                    logistic = logistic = linear_model.LogisticRegression(penalty=self.log_reg_penalty, C=C)\n",
    "                    logistic.fit(x_train, y_train)\n",
    "                    logistic_predictions = logistic.predict(x_test)\n",
    "                    if self.accuracy(logistic_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(logistic_predictions)\n",
    "                        self.log_reg_C = C\n",
    "                print \"Optimal Logistic Regression Hyperparams: Penalty = %s, C = %d.\"\\\n",
    "                    %(self.log_reg_penalty, self.log_reg_C)\n",
    "            logistic = linear_model.LogisticRegression(penalty=self.log_reg_penalty, C = self.log_reg_C)#class_weight='auto'\n",
    "            logistic.fit(x_train, y_train)\n",
    "            logistic_predictions = logistic.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return logistic_predictions, logistic.predict(self.stacking_test_data())\n",
    "            return logistic_predictions\n",
    "    \n",
    "    def lda(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        lda = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "        lda.fit(x_train, y_train)\n",
    "        lda_predictions = lda.predict(x_test)\n",
    "        if self.stacking == True:\n",
    "            return lda_predictions, lda.predict(self.stacking_test_data())\n",
    "        return lda_predictions\n",
    "    \n",
    "    def qda(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            qda = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "            qda.fit(x_train, y_train)\n",
    "            qda_predictions = qda.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return qda_predictions, qda.predict(self.stacking_test_data())            \n",
    "            return qda_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.qda_reg_param == None:\n",
    "                max_accuracy = -1\n",
    "                for reg_param in [x*.1 for x in range(0,10,2)]:\n",
    "                    qda = discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param = reg_param)\n",
    "                    qda.fit(x_train, y_train)\n",
    "                    qda_predictions = qda.predict(x_test)\n",
    "                    if self.accuracy(qda_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(qda_predictions)\n",
    "                        self.qda_reg_param = reg_param\n",
    "                print \"Optimal QDA Hyperparams: Reg_param = %d\" %(self.qda_reg_param)\n",
    "            qda = discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param = self.qda_reg_param)\n",
    "            qda.fit(x_train, y_train)\n",
    "            qda_predictions = qda.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return qda_predictions, qda.predict(self.stacking_test_data())\n",
    "            return qda_predictions\n",
    "    \n",
    "    def svc(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            support_vm = svm.SVC()\n",
    "            support_vm.fit(x_train, y_train)\n",
    "            svm_predictions = support_vm.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return svm_predictions, s.predict(self.stacking_test_data())\n",
    "            return svm_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.svc_C == None:\n",
    "                max_accuracy = -1\n",
    "                for C in [10, 100]:\n",
    "                    s = svm.SVC(C=C)\n",
    "                    s.fit(x_train, y_train)\n",
    "                    svm_predictions = s.predict(x_test)\n",
    "                    if self.accuracy(svm_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(svm_predictions)\n",
    "                        self.svc_C = C\n",
    "            if self.svc_kernel == None:\n",
    "                max_accuracy = -1\n",
    "                for kernel in ['rbf', 'linear', 'poly']:\n",
    "                    s = svm.SVC(C = self.svc_C, kernel=kernel)\n",
    "                    s.fit(x_train, y_train)\n",
    "                    svm_predictions = s.predict(x_test)\n",
    "                    if self.accuracy(svm_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(svm_predictions)\n",
    "                        self.svc_kernel = kernel\n",
    "                print \"Optimal SVM Hyperparams: Kernel = %s, C = %d\" %(self.svc_kernel, self.svc_C)\n",
    "            s = svm.SVC(C = self.svc_C, kernel=self.svc_kernel)\n",
    "            s.fit(x_train, y_train)\n",
    "            svm_predictions = s.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return svm_predictions, s.predict(self.stacking_test_data())\n",
    "            return svm_predictions\n",
    "    \n",
    "    def blend(self, args):\n",
    "        blended_predictions = []\n",
    "        if self.task == 'classification':\n",
    "            n = len(args)\n",
    "            for i in xrange(len(args[0])):\n",
    "                counts = {}\n",
    "                for j in xrange(n):\n",
    "                    try:\n",
    "                        counts[args[j][i]] += 1\n",
    "                    except KeyError:\n",
    "                        counts[args[j][i]] = 1\n",
    "                maximus = -float('inf')\n",
    "                for index, value in counts.iteritems():\n",
    "                    if value > maximus:\n",
    "                        max_value = index\n",
    "                        maximus = value\n",
    "                blended_predictions.append(max_value)\n",
    "        elif self.task == 'regression':\n",
    "            blended_predictions = self.mean(args)\n",
    "        return blended_predictions\n",
    "    \n",
    "    def run_algorithms(self):\n",
    "        if self.task == 'classification':\n",
    "            return [i() for i in self.algorithms]\n",
    "        elif self.task == 'regression':\n",
    "            return [i() for i in self.algorithms]\n",
    "    \n",
    "    def create_predictions(self):\n",
    "        self.testing = True\n",
    "        predictions = (self.run_algorithms() + self.stacker())\n",
    "        self.testing = False\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions).transpose()\n",
    "        self.testing_df = predictions_df\n",
    "        predictions_df.columns = self.algorithm_names\n",
    "        \n",
    "        optimal_predictions = self.blend([predictions_df[j].values for j in self.optimal_iterator])\n",
    "        self.time_elapsed()\n",
    "        return optimal_predictions\n",
    "    \n",
    "    def predict(self):\n",
    "        algorithms = self.run_algorithms()\n",
    "\n",
    "        for i, v in enumerate(algorithms):\n",
    "            print self.accuracy(v), self.algorithm_names[i]\n",
    "        \n",
    "        predictions = (algorithms + self.stacker())\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions).transpose()\n",
    "        self.training_df = predictions_df\n",
    "        predictions_df.columns = self.algorithm_names\n",
    "\n",
    "        combinations = []     \n",
    "        if self.task == 'classification':\n",
    "            col = predictions_df.columns\n",
    "            for i in xrange(1, len(col), 2):\n",
    "                for j in itertools.combinations(col, i):\n",
    "                    combinations.append([k for k in j])            \n",
    "            maximus = -1\n",
    "            for i in combinations:\n",
    "                blended = self.blend([predictions_df[j].values for j in i])\n",
    "                accuracy = self.accuracy(blended)\n",
    "                if accuracy > maximus:\n",
    "                    maximus = accuracy\n",
    "                    optimal_predictions = blended\n",
    "                    self.optimal_iterator = i\n",
    "                    print str(accuracy) + str(i)            \n",
    "        elif self.task == 'regression':\n",
    "            col = predictions_df.columns\n",
    "            for i in xrange(1, len(col)):\n",
    "                for j in itertools.combinations(col, i):\n",
    "                    combinations.append([k for k in j])            \n",
    "            maximus = float(\"inf\")\n",
    "            for i in combinations:\n",
    "                blended = self.blend([predictions_df[j].values for j in i])\n",
    "                accuracy = self.accuracy(blended)\n",
    "                if accuracy < maximus:\n",
    "                    maximus = accuracy\n",
    "                    optimal_predictions = blended\n",
    "                    self.optimal_iterator = i\n",
    "                    print str(accuracy) + str(i)\n",
    "        \n",
    "        if self.test != None: \n",
    "            return self.create_predictions()\n",
    "        else:\n",
    "            self.time_elapsed()\n",
    "            return optimal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.127371331579 linear_predictions\n",
      "0.12351153089 ridge_predictions\n",
      "0.688312085527 lasso_predictions\n",
      "0.171884002777 booster_predictions\n",
      "0.105875 forest_predictions\n",
      "0.0989375 erf_predictions\n",
      "0.127371331579['linear_predictions']\n",
      "0.12351153089['ridge_predictions']\n",
      "0.105875['forest_predictions']\n",
      "0.0989375['erf_predictions']\n",
      "0.0948125['erf_on_stack']\n",
      "0.090625['forest_predictions', 'erf_predictions']\n",
      "0.090480952381['forest_predictions', 'erf_predictions', 'erf_on_stack']\n",
      "Time Elapsed = 0h 0m 5s\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(path + \"iris.data\", header=None)\n",
    "y = iris[3]\n",
    "iris = iris.drop([3], 1)\n",
    "iris = iris.drop([4], 1)\n",
    "problem = apm(iris, y, task='regression', hyperparameters='default')\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.897673879197\n",
      "1 1.0286112809\n",
      "2 -1.33679402029\n",
      "3 0.407516490679\n",
      "4 2.7971786408\n",
      "5 0.262686487586\n",
      "6 0.471404520791\n",
      "7 0.790569415042\n",
      "8 0.260377821962\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(problem.x.shape[1]):\n",
    "    print i, problem.x[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4],\n",
       "       [ 4.9,  3. ,  1.4],\n",
       "       [ 4.7,  3.2,  1.3],\n",
       "       [ 4.6,  3.1,  1.5],\n",
       "       [ 5. ,  3.6,  1.4],\n",
       "       [ 5.4,  3.9,  1.7],\n",
       "       [ 4.6,  3.4,  1.4],\n",
       "       [ 5. ,  3.4,  1.5],\n",
       "       [ 4.4,  2.9,  1.4],\n",
       "       [ 4.9,  3.1,  1.5],\n",
       "       [ 5.4,  3.7,  1.5],\n",
       "       [ 4.8,  3.4,  1.6],\n",
       "       [ 4.8,  3. ,  1.4],\n",
       "       [ 4.3,  3. ,  1.1],\n",
       "       [ 5.8,  4. ,  1.2],\n",
       "       [ 5.7,  4.4,  1.5],\n",
       "       [ 5.4,  3.9,  1.3],\n",
       "       [ 5.1,  3.5,  1.4],\n",
       "       [ 5.7,  3.8,  1.7],\n",
       "       [ 5.1,  3.8,  1.5],\n",
       "       [ 5.4,  3.4,  1.7],\n",
       "       [ 5.1,  3.7,  1.5],\n",
       "       [ 4.6,  3.6,  1. ],\n",
       "       [ 5.1,  3.3,  1.7],\n",
       "       [ 4.8,  3.4,  1.9],\n",
       "       [ 5. ,  3. ,  1.6],\n",
       "       [ 5. ,  3.4,  1.6],\n",
       "       [ 5.2,  3.5,  1.5],\n",
       "       [ 5.2,  3.4,  1.4],\n",
       "       [ 4.7,  3.2,  1.6],\n",
       "       [ 4.8,  3.1,  1.6],\n",
       "       [ 5.4,  3.4,  1.5],\n",
       "       [ 5.2,  4.1,  1.5],\n",
       "       [ 5.5,  4.2,  1.4],\n",
       "       [ 4.9,  3.1,  1.5],\n",
       "       [ 5. ,  3.2,  1.2],\n",
       "       [ 5.5,  3.5,  1.3],\n",
       "       [ 4.9,  3.1,  1.5],\n",
       "       [ 4.4,  3. ,  1.3],\n",
       "       [ 5.1,  3.4,  1.5],\n",
       "       [ 5. ,  3.5,  1.3],\n",
       "       [ 4.5,  2.3,  1.3],\n",
       "       [ 4.4,  3.2,  1.3],\n",
       "       [ 5. ,  3.5,  1.6],\n",
       "       [ 5.1,  3.8,  1.9],\n",
       "       [ 4.8,  3. ,  1.4],\n",
       "       [ 5.1,  3.8,  1.6],\n",
       "       [ 4.6,  3.2,  1.4],\n",
       "       [ 5.3,  3.7,  1.5],\n",
       "       [ 5. ,  3.3,  1.4],\n",
       "       [ 7. ,  3.2,  4.7],\n",
       "       [ 6.4,  3.2,  4.5],\n",
       "       [ 6.9,  3.1,  4.9],\n",
       "       [ 5.5,  2.3,  4. ],\n",
       "       [ 6.5,  2.8,  4.6],\n",
       "       [ 5.7,  2.8,  4.5],\n",
       "       [ 6.3,  3.3,  4.7],\n",
       "       [ 4.9,  2.4,  3.3],\n",
       "       [ 6.6,  2.9,  4.6],\n",
       "       [ 5.2,  2.7,  3.9],\n",
       "       [ 5. ,  2. ,  3.5],\n",
       "       [ 5.9,  3. ,  4.2],\n",
       "       [ 6. ,  2.2,  4. ],\n",
       "       [ 6.1,  2.9,  4.7],\n",
       "       [ 5.6,  2.9,  3.6],\n",
       "       [ 6.7,  3.1,  4.4],\n",
       "       [ 5.6,  3. ,  4.5],\n",
       "       [ 5.8,  2.7,  4.1],\n",
       "       [ 6.2,  2.2,  4.5],\n",
       "       [ 5.6,  2.5,  3.9],\n",
       "       [ 5.9,  3.2,  4.8],\n",
       "       [ 6.1,  2.8,  4. ],\n",
       "       [ 6.3,  2.5,  4.9],\n",
       "       [ 6.1,  2.8,  4.7],\n",
       "       [ 6.4,  2.9,  4.3],\n",
       "       [ 6.6,  3. ,  4.4],\n",
       "       [ 6.8,  2.8,  4.8],\n",
       "       [ 6.7,  3. ,  5. ],\n",
       "       [ 6. ,  2.9,  4.5],\n",
       "       [ 5.7,  2.6,  3.5],\n",
       "       [ 5.5,  2.4,  3.8],\n",
       "       [ 5.5,  2.4,  3.7],\n",
       "       [ 5.8,  2.7,  3.9],\n",
       "       [ 6. ,  2.7,  5.1],\n",
       "       [ 5.4,  3. ,  4.5],\n",
       "       [ 6. ,  3.4,  4.5],\n",
       "       [ 6.7,  3.1,  4.7],\n",
       "       [ 6.3,  2.3,  4.4],\n",
       "       [ 5.6,  3. ,  4.1],\n",
       "       [ 5.5,  2.5,  4. ],\n",
       "       [ 5.5,  2.6,  4.4],\n",
       "       [ 6.1,  3. ,  4.6],\n",
       "       [ 5.8,  2.6,  4. ],\n",
       "       [ 5. ,  2.3,  3.3],\n",
       "       [ 5.6,  2.7,  4.2],\n",
       "       [ 5.7,  3. ,  4.2],\n",
       "       [ 5.7,  2.9,  4.2],\n",
       "       [ 6.2,  2.9,  4.3],\n",
       "       [ 5.1,  2.5,  3. ],\n",
       "       [ 5.7,  2.8,  4.1],\n",
       "       [ 6.3,  3.3,  6. ],\n",
       "       [ 5.8,  2.7,  5.1],\n",
       "       [ 7.1,  3. ,  5.9],\n",
       "       [ 6.3,  2.9,  5.6],\n",
       "       [ 6.5,  3. ,  5.8],\n",
       "       [ 7.6,  3. ,  6.6],\n",
       "       [ 4.9,  2.5,  4.5],\n",
       "       [ 7.3,  2.9,  6.3],\n",
       "       [ 6.7,  2.5,  5.8],\n",
       "       [ 7.2,  3.6,  6.1],\n",
       "       [ 6.5,  3.2,  5.1],\n",
       "       [ 6.4,  2.7,  5.3],\n",
       "       [ 6.8,  3. ,  5.5],\n",
       "       [ 5.7,  2.5,  5. ],\n",
       "       [ 5.8,  2.8,  5.1],\n",
       "       [ 6.4,  3.2,  5.3],\n",
       "       [ 6.5,  3. ,  5.5],\n",
       "       [ 7.7,  3.8,  6.7],\n",
       "       [ 7.7,  2.6,  6.9],\n",
       "       [ 6. ,  2.2,  5. ],\n",
       "       [ 6.9,  3.2,  5.7],\n",
       "       [ 5.6,  2.8,  4.9],\n",
       "       [ 7.7,  2.8,  6.7],\n",
       "       [ 6.3,  2.7,  4.9],\n",
       "       [ 6.7,  3.3,  5.7],\n",
       "       [ 7.2,  3.2,  6. ],\n",
       "       [ 6.2,  2.8,  4.8],\n",
       "       [ 6.1,  3. ,  4.9],\n",
       "       [ 6.4,  2.8,  5.6],\n",
       "       [ 7.2,  3. ,  5.8],\n",
       "       [ 7.4,  2.8,  6.1],\n",
       "       [ 7.9,  3.8,  6.4],\n",
       "       [ 6.4,  2.8,  5.6],\n",
       "       [ 6.3,  2.8,  5.1],\n",
       "       [ 6.1,  2.6,  5.6],\n",
       "       [ 7.7,  3. ,  6.1],\n",
       "       [ 6.3,  3.4,  5.6],\n",
       "       [ 6.4,  3.1,  5.5],\n",
       "       [ 6. ,  3. ,  4.8],\n",
       "       [ 6.9,  3.1,  5.4],\n",
       "       [ 6.7,  3.1,  5.6],\n",
       "       [ 6.9,  3.1,  5.1],\n",
       "       [ 5.8,  2.7,  5.1],\n",
       "       [ 6.8,  3.2,  5.9],\n",
       "       [ 6.7,  3.3,  5.7],\n",
       "       [ 6.7,  3. ,  5.2],\n",
       "       [ 6.3,  2.5,  5. ],\n",
       "       [ 6.5,  3. ,  5.2],\n",
       "       [ 6.2,  3.4,  5.4],\n",
       "       [ 5.9,  3. ,  5.1]])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris:\n",
      "Optimal Booster Hyperparams: n_estimators = 100, max_depth = 2\n",
      "Optimal erf Hyperparams: n_estimators = 100, max_features = 3\n",
      "Optimal Logistic Regression Hyperparams: Penalty = l1, C = 1.\n",
      "Optimal QDA Hyperparams: Reg_param = 0\n",
      "1.0 xgboost_predictions\n",
      "1.0 booster_predictions\n",
      "1.0 forest_predictions\n",
      "1.0 erf_predictions\n",
      "1.0 logistic_predictions\n",
      "0.966666666667 lda_predictions\n",
      "1.0 qda_predictions\n",
      "1.0['xgboost_predictions']\n",
      "Time Elapsed = 0h 0m 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnoxon/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:612: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(path + \"iris.data\", header=None)\n",
    "y = iris[4]\n",
    "iris = iris.drop([4], 1)\n",
    "print \"Iris:\"\n",
    "\n",
    "problem = apm(iris, y, task='classification', hyperparameters='individual', test = iris[:20], debug=True)\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540625 xgboost_predictions\n",
      "0.653125 booster_predictions\n",
      "0.646875 forest_predictions\n",
      "0.665625 erf_predictions\n",
      "0.55625 logistic_predictions\n",
      "0.546875 lda_predictions\n",
      "0.565625 qda_predictions\n",
      "0.540625['xgboost_predictions']\n",
      "0.653125['booster_predictions']\n",
      "0.665625['erf_predictions']\n",
      "0.66875['xgboost_predictions', 'erf_predictions', 'lda_predictions']\n",
      "0.671875['booster_predictions', 'erf_predictions', 'forest_on_stack']\n",
      "0.675['xgboost_predictions', 'booster_predictions', 'forest_predictions', 'erf_predictions', 'lda_predictions']\n",
      "0.6875['xgboost_predictions', 'booster_predictions', 'forest_predictions', 'erf_predictions', 'qda_predictions']\n",
      "0.690625['xgboost_predictions', 'booster_predictions', 'erf_predictions', 'lda_predictions', 'xgboost_on_stack', 'booster_on_stack', 'forest_on_stack']\n",
      "Time Elapsed = 0h 0m 32s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "Y = df['quality'].values\n",
    "df = df.drop('quality',1)\n",
    "problem = apm(df, Y, task='classification', hyperparameters='default')\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate(hyperparameters='individual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "model_permutations = []\n",
    "models = [problem.xgboost, problem.gradient_booster, problem.random_forest,\\\n",
    "          problem.extremely_random_forest, problem.logistic_regression, problem.lda, problem.qda]\n",
    "testing = ['xgboost', 'grad_boost', 'rf', 'erf', 'lr', 'lda', 'qda']\n",
    "for j in itertools.permutations(models, 2):\n",
    "    model_permutations.append([k for k in j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "for j in itertools.permutations(testing, 2):\n",
    "    test.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method classifier.xgboost of <__main__.classifier instance at 0x10efc8a28>>,\n",
       " <bound method classifier.gradient_booster of <__main__.classifier instance at 0x10efc8a28>>]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_permutations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wine Quality:\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.gradient_booster of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "Optimal Booster Hyperparams: n_estimators = 100, max_depth = 8\n",
      "<bound method classifier.random_forest of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "Optimal Forest Hyperparams: n_estimators = 900, max_features = 4\n",
      "<bound method classifier.extremely_random_forest of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "Optimal erf Hyperparams: n_estimators = 100, proportion = 4\n",
      "<bound method classifier.logistic_regression of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "Optimal Logistic Regression Hyperparams: Penalty = l2, C = 1.\n",
      "<bound method classifier.lda of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.qda of <__main__.classifier instance at 0x10e4ac878>>\n",
      "<bound method classifier.xgboost of <__main__.classifier instance at 0x10e4ac878>>\n",
      "Optimal QDA Hyperparams: Reg_param = 0\n",
      "Optimal Booster Hyperparams: n_estimators = 1000, max_depth = 8\n",
      "Optimal Forest Hyperparams: n_estimators = 500, max_features = 9\n",
      "Optimal erf Hyperparams: n_estimators = 500, proportion = 7\n",
      "Optimal Logistic Regression Hyperparams: Penalty = l2, C = 1.\n",
      "Optimal QDA Hyperparams: Reg_param = 0\n",
      "[0.55625, 0.665625, 0.665625, 0.646875, 0.615625, 0.596875, 0.625]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path + 'Oracle Development/winequality-red.csv', sep=';')\n",
    "Y = df['quality'].values\n",
    "df = df.drop('quality',1)\n",
    "print\"\\nWine Quality:\"\n",
    "\n",
    "problem = classifier(df, Y, hyperparameters='individual')\n",
    "predictions = problem.stacker()\n",
    "print [problem.accuracy(prediction) for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wine Quality:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.653125"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + 'Oracle Development/winequality-red.csv', sep=';')\n",
    "Y = df['quality'].values\n",
    "df = df.drop('quality',1)\n",
    "print\"\\nWine Quality:\"\n",
    "\n",
    "problem = classifier(df, Y, hyperparameters='default')\n",
    "predictions = problem.gradient_booster()\n",
    "problem.accuracy(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wine Quality:\n",
      "Optimal Booster Hyperparams: n_estimators = 700, max_depth = 5\n",
      "Optimal Forest Hyperparams: n_estimators = 100, max_features = 3\n",
      "Optimal erf Hyperparams: n_estimators = 100, proportion = 4\n",
      "Optimal Logistic Regression Hyperparams: Penalty = l1, C = 1.\n",
      "Optimal QDA Hyperparams: Reg_param = 0\n",
      "0.625 xgboost_predictions\n",
      "0.65625 booster_predictions\n",
      "0.640625 forest_predictions\n",
      "0.678125 erf_predictions\n",
      "0.575 logistic_predictions\n",
      "0.559375 lda_predictions\n",
      "0.55625 qda_predictions\n",
      "0.6625 blended_predictions\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path + 'Oracle Development/winequality-red.csv', sep=';')\n",
    "Y = df['quality'].values\n",
    "df = df.drop('quality',1)\n",
    "print\"\\nWine Quality:\"\n",
    "\n",
    "problem = classifier(df, Y, hyperparameters='individual')\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Leads Met:\n",
      "Optimal Booster Hyperparams: n_estimators = 100, max_depth = 2\n",
      "Optimal Forest Hyperparams: n_estimators = 100, max_features = 23\n",
      "Optimal erf Hyperparams: n_estimators = 500, proportion = 23\n",
      "Optimal Logistic Regression Hyperparams: Penalty = l1, C = 53.\n",
      "Optimal QDA Hyperparams: Reg_param = 0\n",
      "0.130831973899 xgboost_predictions\n",
      "0.868515497553 booster_predictions\n",
      "0.862316476346 forest_predictions\n",
      "0.86101141925 erf_predictions\n",
      "0.869004893964 logistic_predictions\n",
      "0.869004893964 lda_predictions\n",
      "0.544861337684 qda_predictions\n",
      "Optimal Booster Hyperparams: n_estimators = 100, max_depth = 2\n",
      "Optimal Forest Hyperparams: n_estimators = 900, max_features = 31\n",
      "Optimal erf Hyperparams: n_estimators = 900, proportion = 23\n",
      "Optimal Logistic Regression Hyperparams: Penalty = l1, C = 1.\n",
      "Optimal QDA Hyperparams: Reg_param = 0\n",
      "Optimal Booster Hyperparams: n_estimators = 100, max_depth = 2\n",
      "Optimal Forest Hyperparams: n_estimators = 500, max_features = 40\n",
      "Optimal erf Hyperparams: n_estimators = 500, proportion = 30\n",
      "Optimal Logistic Regression Hyperparams: Penalty = l1, C = 1.\n",
      "Optimal QDA Hyperparams: Reg_param = 0\n",
      "0.130831973899['xgboost_predictions']\n",
      "0.868515497553['booster_predictions']\n",
      "0.869004893964['logistic_predictions']\n",
      "0.869331158238['booster_predictions', 'logistic_predictions', 'lda_on_stack']\n",
      "0.869657422512['booster_predictions', 'forest_on_stack', 'lda_on_stack']\n",
      "0.869983686786['booster_predictions', 'erf_on_stack', 'lda_on_stack']\n",
      "0.870636215334['xgboost_predictions', 'booster_predictions', 'logistic_predictions', 'forest_on_stack', 'lda_on_stack']\n",
      "0.870799347471['erf_predictions', 'logistic_predictions', 'lda_predictions', 'qda_predictions', 'booster_on_stack', 'erf_on_stack', 'lda_on_stack']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-db0e7ee30661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mproblem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'individual'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "leads = pd.read_csv(path + 'Movoto/leads.csv')\n",
    "one_hot_columns = ['First_Lead_Platform', 'State']\n",
    "for i in one_hot_columns:\n",
    "    dummies = pd.get_dummies(leads[i])\n",
    "    for j in dummies:\n",
    "        leads[j] = dummies[j]\n",
    "y = leads['Met']\n",
    "leads = leads.drop(['First_Lead_Platform', 'State', 'Met'], axis =1)\n",
    "print\"\\nLeads Met:\"\n",
    "\n",
    "problem = classifier(leads, y, hyperparameters='individual')\n",
    "predictions, optimal = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leads = pd.read_csv(path + 'Movoto/leads.csv')\n",
    "one_hot_columns = ['First_Lead_Platform', 'State']\n",
    "for i in one_hot_columns:\n",
    "    dummies = pd.get_dummies(leads[i])\n",
    "    for j in dummies:\n",
    "        leads[j] = dummies[j]\n",
    "y = leads['Met']\n",
    "leads = leads.drop(['First_Lead_Platform', 'State', 'Met'], axis =1)\n",
    "print\"\\nLeads Met:\"\n",
    "\n",
    "problem = classifier(leads, y, hyperparameters='individual')\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgboost_predictions</th>\n",
       "      <th>booster_predictions</th>\n",
       "      <th>forest_predictions</th>\n",
       "      <th>erf_predictions</th>\n",
       "      <th>logistic_predictions</th>\n",
       "      <th>lda_predictions</th>\n",
       "      <th>qda_predictions</th>\n",
       "      <th>blended_predictions</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   xgboost_predictions  booster_predictions  forest_predictions  \\\n",
       "0                    1                    0                   0   \n",
       "1                    1                    0                   0   \n",
       "2                    1                    0                   0   \n",
       "3                    1                    0                   0   \n",
       "4                    1                    0                   0   \n",
       "\n",
       "   erf_predictions  logistic_predictions  lda_predictions  qda_predictions  \\\n",
       "0                0                     1                1                1   \n",
       "1                0                     0                0                0   \n",
       "2                0                     0                0                0   \n",
       "3                0                     0                0                0   \n",
       "4                0                     0                0                0   \n",
       "\n",
       "   blended_predictions  y_test  \n",
       "0                    1       1  \n",
       "1                    0       1  \n",
       "2                    0       1  \n",
       "3                    0       0  \n",
       "4                    0       0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictions_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path + 'Oracle Development/winequality-red.csv', sep=';')\n",
    "# Y = df['quality'].values\n",
    "# df = df.drop('quality',1)\n",
    "# print\"\\nWine Quality:\"\n",
    "\n",
    "# problem = classifier(df, Y, hyperparameters='individual')\n",
    "# predictions, y_test = problem.predict()\n",
    "\n",
    "# predictions_df = pd.DataFrame(predictions).transpose()\n",
    "# predictions_df.columns = ['xgboost_predictions','booster_predictions', 'forest_predictions', 'erf_predictions',\\\n",
    "#                   'logistic_predictions', 'lda_predictions', 'qda_predictions', \\\n",
    "#                   'blended_predictions', 'xgboost_on_stack', 'booster_on_stack', \\\n",
    "#                     'forest_on_stack', 'erf_on_stack', 'logistic_on_stack', 'lda_on_stack', \\\n",
    "#                     'qda_on_stack']\n",
    "\n",
    "# import itertools\n",
    "# permutations = []\n",
    "# col = predictions_df.columns[:-1]\n",
    "# for i in xrange(1, len(col), 2):\n",
    "#     for j in itertools.combinations(col, i):\n",
    "#         permutations.append([k for k in j])\n",
    "\n",
    "# maximus = -1\n",
    "# for i in permutations:\n",
    "#     accuracy = problem.accuracy(blend([predictions_df[j].values for j in i]))\n",
    "#     if accuracy >= maximus:\n",
    "#         maximus = accuracy\n",
    "#         print str(accuracy) + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictions_df.columns = ['xgboost_predictions','booster_predictions', 'forest_predictions', 'erf_predictions',\\\n",
    "#                   'logistic_predictions', 'lda_predictions', 'qda_predictions', \\\n",
    "#                   'blended_predictions', 'xgboost_on_stack', 'booster_on_stack', \\\n",
    "#                     'forest_on_stack', 'erf_on_stack', 'logistic_on_stack', 'lda_on_stack', \\\n",
    "#                     'qda_on_stack']\n",
    "\n",
    "# import itertools\n",
    "# permutations = []\n",
    "# col = predictions_df.columns[:-1]\n",
    "# for i in xrange(1, len(col), 2):\n",
    "#     for j in itertools.combinations(col, i):\n",
    "#         permutations.append([k for k in j])\n",
    "\n",
    "# maximus = -1\n",
    "# for i in permutations:\n",
    "#     accuracy = problem.accuracy(blend([predictions_df[j].values for j in i]))\n",
    "#     if accuracy >= maximus:\n",
    "#         maximus = accuracy\n",
    "#         print str(accuracy) + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
