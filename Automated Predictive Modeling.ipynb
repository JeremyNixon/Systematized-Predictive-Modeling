{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import itertools\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "path = home + \"/Dropbox/python/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { \n",
       "\n",
       "  width:100% !important;\n",
       "  background-color: #050505 !important;\n",
       " }\n",
       "/*body {background-color: #1b202b;}*/\n",
       "\n",
       "\n",
       "/* GLOBALS */\n",
       "body {background-color: #050505; color: #777;}\n",
       "a {color: #8fa1b3;}\n",
       "\n",
       "/* INTRO PAGE */\n",
       ".toolbar_info, .list_container {color: #050505;}\n",
       "\n",
       "/* NOTEBOOK */\n",
       "\n",
       "/* comment out this line to bring the toolbar back */\n",
       "/*div#maintoolbar, div#header {display: none !important;}\n",
       "*/\n",
       "\n",
       "div.header {background-color: #050505;}\n",
       "div.lower-header-bar {background-color: #1b202b;}\n",
       "\n",
       "div#maintoolbar {background-color: #050505;}\n",
       "div#maintoolbar.navbar {background-color: #050505;}\n",
       "\n",
       "div#notebook {border-top: none; background-color: #050505;}\n",
       "div.navbar-collapse {background-color: #050505;}\n",
       "div#maintoolbar-container {display: none !important; background-color: 050505;}\n",
       "/*div#header-container {display: none !important;}\n",
       "*/\n",
       "\n",
       "div.input_prompt {color: #050505;}\n",
       "div.output_prompt {color: #050505;}\n",
       "div.input_area {\n",
       "  border-radius: 0px;\n",
       "  border: 1px solid #4f5b66;\n",
       "}\n",
       "div.output_area pre {font-weight: normal; color: #c0c5ce;}\n",
       "div.output_subarea {font-weight: normal; color: #c0c5ce;}\n",
       "\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "  border: 1px  #c0c5ce solid;\n",
       "  color: #c0c5ce;\n",
       "}\n",
       "div.output_html { font-family: sans-serif; }\n",
       "table.dataframe tr {border: 1px #c0c5ce;}\n",
       "\n",
       "div.cell.selected {border-radius: 0px;}\n",
       "div.cell.edit_mode {border-radius: 0px; border: thin solid #b48ead;}\n",
       "div.text_cell_render, div.output_html {color: #c0c5ce;}\n",
       "\n",
       "span.ansiblack {color: #343d46;}\n",
       "span.ansiblue {color: #96b5b4;}\n",
       "span.ansigray {color: #a7adba;}\n",
       "span.ansigreen {color: #a3be8c;}\n",
       "span.ansipurple {color: #b48ead;}\n",
       "span.ansired {color: #bf616a;}\n",
       "span.ansiyellow {color: #ebcb8b;}\n",
       "\n",
       "div.output_stderr {background-color: #050505;}\n",
       "div.output_stderr pre {color: #dfe1e8;}\n",
       "\n",
       ".cm-s-ipython.CodeMirror {background: #050505; color: #dfe1e8;}\n",
       ".cm-s-ipython div.CodeMirror-selected {background: #343d46 !important;}\n",
       ".cm-s-ipython .CodeMirror-gutters {background: #2b303b; border-right: 0px;}\n",
       ".cm-s-ipython .CodeMirror-linenumber {color: #65737e;}\n",
       ".cm-s-ipython .CodeMirror-cursor {border-left: 1px solid #a7adba !important;}\n",
       "\n",
       ".cm-s-ipython span.cm-comment {color: #ab7967;}\n",
       ".cm-s-ipython span.cm-atom {color: #b48ead;}\n",
       ".cm-s-ipython span.cm-number {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython span.cm-property, .cm-s-ipython span.cm-attribute {color: #a3be8c;}\n",
       ".cm-s-ipython span.cm-keyword {color: #bf616a;}\n",
       ".cm-s-ipython span.cm-string {color: #ebcb8b;}\n",
       ".cm-s-ipython span.cm-operator {color: #ab7967;}\n",
       ".cm-s-ipython span.cm-builtin {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython span.cm-variable {color: #a3be8c;}\n",
       ".cm-s-ipython span.cm-variable-2 {color: #8fa1b3;}\n",
       ".cm-s-ipython span.cm-def {color: #d08770;}\n",
       ".cm-s-ipython span.cm-error {background: #bf616a; color: #a7adba;}\n",
       ".cm-s-ipython span.cm-bracket {color: #c0c5ce;}\n",
       ".cm-s-ipython span.cm-tag {color: #bf616a;}\n",
       ".cm-s-ipython span.cm-link {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython .CodeMirror-matchingbracket { text-decoration: underline; color: #dfe1e8 !important;}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"/Users/jnoxon/Dropbox/python/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "path = home + \"/Dropbox/python/\"\n",
    "iris = pd.read_csv(\"iris.data\", header=None)\n",
    "y = iris[4]\n",
    "iris = iris.drop([4], 1)\n",
    "x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(iris, y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "Y = df['quality'].values\n",
    "df = df.drop('quality',1)\n",
    "# x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(df, Y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('breast_cancer.csv')\n",
    "#http://www.is.umk.pl/projects/datasets.html\n",
    "y = np.array(cancer['2.1'])\n",
    "x = np.array(cancer.drop(['2.1', '1.3'], 1))\n",
    "# x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(x, y, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "votes = pd.read_csv('votes.csv')\n",
    "votes = votes.replace('?', float('NaN'))\n",
    "votes = votes.dropna()\n",
    "votes = votes.reset_index(drop=True)\n",
    "votes['0'] = votes.replace('D', 1)\n",
    "votes['0'] = votes.replace('R', 0)\n",
    "y = np.array(votes['0'])\n",
    "votes = votes.drop('0', axis = 1)\n",
    "votes = votes.replace('y', 1)\n",
    "votes = votes.replace('n', 0)\n",
    "x = np.array(votes)\n",
    "# x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(np.array(x), np.array(y), test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/jeremynixon/Dropbox/Kiko/w2v_train.csv\", header=None)\n",
    "labels = pd.read_csv(\"/Users/jeremynixon/Dropbox/Kiko/labels.csv\", header=None)\n",
    "x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(train, labels, test_size = .20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(hyperparameters='individual'):\n",
    "    iris = pd.read_csv(\"iris.data\", header=None)\n",
    "    y = iris[4]\n",
    "    iris = iris.drop([4], 1)\n",
    "    print \"Iris:\"\n",
    "    \n",
    "    problem = classifier(iris,y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "    Y = df['quality'].values\n",
    "    df = df.drop('quality',1)\n",
    "    print\"\\nWine Quality:\"\n",
    "    \n",
    "    problem = classifier(df, Y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    cancer = pd.read_csv('breast_cancer.csv')\n",
    "    #http://www.is.umk.pl/projects/datasets.html\n",
    "    y = np.array(cancer['2.1'])\n",
    "    x = np.array(cancer.drop(['2.1', '1.3'], 1))\n",
    "    print \"\\nBreast Cancer:\"\n",
    "    \n",
    "    problem = classifier(x, y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    votes = pd.read_csv('votes.csv')\n",
    "    votes = votes.replace('?', float('NaN'))\n",
    "    votes = votes.dropna()\n",
    "    votes = votes.reset_index(drop=True)\n",
    "    votes['0'] = votes.replace('D', 1)\n",
    "    votes['0'] = votes.replace('R', 0)\n",
    "    y = np.array(votes['0'])\n",
    "    votes = votes.drop('0', axis = 1)\n",
    "    votes = votes.replace('y', 1)\n",
    "    votes = votes.replace('n', 0)\n",
    "    x = np.array(votes)\n",
    "    print \"\\nVotes:\"\n",
    "    \n",
    "    problem = classifier(x, y, hyperparameters=hyperparameters)\n",
    "    problem.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import itertools\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.cross_validation\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "class apm():\n",
    "    def __init__(self, x, y, task, test=None, hyperparameters='individual', debug=False):\n",
    "        self.time = time.time()\n",
    "        self.task = task\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.x = np.array(x)\n",
    "        self.y = y\n",
    "        if self.task == 'classification':\n",
    "            if type(self.y[0]) != int:\n",
    "                self.y, self.mapping = self.categorize(self.y)\n",
    "            self.y = np.array(self.y, dtype = int)\n",
    "        elif self.task == 'regression':\n",
    "            self.y = np.array(self.y, dtype = float)\n",
    "        if test is None:\n",
    "            self.test = None\n",
    "        else:\n",
    "            self.test = self.feature_transform(np.array(test))\n",
    "            \n",
    "        self.x = self.feature_transform(self.x)\n",
    "        \n",
    "        \n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = sklearn.cross_validation.train_test_split\\\n",
    "            (self.x, self.y, test_size = .20, random_state=42)\n",
    "        self.x_train1 = self.x_train[:len(self.x_train)/2]\n",
    "        self.x_train2 = self.x_train[len(self.x_train)/2:]\n",
    "        self.y_train1 = self.y_train[:len(self.x_train)/2]\n",
    "        self.y_train2 = self.y_train[len(self.x_train)/2:]\n",
    "        self.stacking_train = self.x_train2\n",
    "        self.stacking_test = self.x_test\n",
    "        \n",
    "        self.x1 = self.x[:len(self.x)/2]\n",
    "        self.x2 = self.x[len(self.x)/2:]\n",
    "        self.y1 = self.y[:len(self.x)/2]\n",
    "        self.y2 = self.y[len(self.x)/2:]\n",
    "        self.test_stacking_train = self.x2\n",
    "        self.test_stacking_test = self.test\n",
    "\n",
    "        self.debug = False\n",
    "        self.stacking = False\n",
    "        self.testing = False\n",
    "        self.optimal_iterator = None\n",
    "        \n",
    "        if self.task == 'classification':\n",
    "            if self.debug:\n",
    "                self.algorithms = [self.logistic_regression]\n",
    "                self.algorithm_names = ['logistic_predictions']\n",
    "            else:\n",
    "                self.algorithms = [self.xgboost, self.gradient_booster, self.random_forest,\\\n",
    "                      self.extremely_random_forest, self.logistic_regression, self.lda, self.qda]\n",
    "                self.algorithm_names = ['xgboost_predictions','booster_predictions', 'forest_predictions', 'erf_predictions',\\\n",
    "                          'logistic_predictions', 'lda_predictions', 'qda_predictions', \\\n",
    "                           'xgboost_on_stack', 'booster_on_stack','forest_on_stack', \\\n",
    "                            'erf_on_stack', 'logistic_on_stack', 'lda_on_stack', 'qda_on_stack']\n",
    "        elif self.task == 'regression':\n",
    "            if self.debug:\n",
    "                self.algorithms = [self.linear_regression, self.ridge_regression, self.lasso_regression,\\\n",
    "                                   self.gradient_booster, self.random_forest, self.extremely_random_forest]\n",
    "                self.algorithm_names = ['linear_predictions', 'ridge_predictions', 'lasso_predictions',\\\n",
    "                                        'booster_predictions', 'forest_predictions', 'erf_predictions', 'linear_on_stack',\\\n",
    "                                        'ridge_on_stack', 'lasso_on_stack', 'booster_on_stack', 'forest_on_stack', 'erf_on_stack']\n",
    "            else:\n",
    "                self.algorithms = [self.linear_regression, self.ridge_regression, self.lasso_regression,\\\n",
    "                                   self.gradient_booster, self.random_forest, self.extremely_random_forest]\n",
    "                self.algorithm_names = ['linear_predictions', 'ridge_predictions', 'lasso_predictions',\\\n",
    "                                        'booster_predictions', 'forest_predictions', 'erf_predictions', 'linear_on_stack',\\\n",
    "                                        'ridge_on_stack', 'lasso_on_stack', 'booster_on_stack', 'forest_on_stack', 'erf_on_stack']\n",
    "                \n",
    "        self.xgb_max_depth = None\n",
    "        self.gb_n_estimators = None\n",
    "        self.gb_max_depth = None\n",
    "        self.rf_n_estimators = 400\n",
    "        self.rf_max_features = \"auto\"\n",
    "        self.erf_n_estimators = None\n",
    "        self.erf_max_features = None\n",
    "        self.log_reg_penalty = None\n",
    "        self.log_reg_C = None\n",
    "        self.qda_reg_param = None\n",
    "        self.svc_C = None\n",
    "        self.svc_kernel = None\n",
    "        \n",
    "        self.training_df = None\n",
    "        self.testing_df = None        \n",
    "        \n",
    "    \n",
    "    def accuracy(self, predictions):\n",
    "        if len(predictions) == len(self.y_test):\n",
    "            y_test = self.y_test\n",
    "        elif len(predictions) == len(self.y_train2):\n",
    "            y_test = self.y_train2\n",
    "        elif len(predictions) == len(self.y2):\n",
    "            y_test = self.y2\n",
    "        else:\n",
    "            print \"Accuracy called with incorrectly sized predictions\"\n",
    "        if self.task == 'classification':\n",
    "            count = 0\n",
    "            for i in xrange(len(predictions)):\n",
    "                if predictions[i] == y_test[i]:\n",
    "                    count += 1\n",
    "            accuracy = count/float(len(predictions))\n",
    "        elif self.task == 'regression':\n",
    "            error = [np.abs(predictions[i] - y_test[i]) for i in xrange(len(predictions))]\n",
    "            accuracy = np.median(error)\n",
    "        return accuracy\n",
    "    \n",
    "    def categorize(self, labels):\n",
    "        mapping = {}\n",
    "        labels = pd.Series(labels)\n",
    "        for i,v in enumerate(list(set(labels))):\n",
    "            labels = labels.replace(v, i)\n",
    "            mapping[i] = v\n",
    "        return np.array(labels), mapping  \n",
    "    \n",
    "    def stacker(self):\n",
    "        self.stacking = True\n",
    "        self.train_stacker()\n",
    "        self.stacking = False\n",
    "        return self.test_stacker()\n",
    "    \n",
    "    def time_elapsed(self):\n",
    "        seconds = time.time()-self.time\n",
    "        hours = int(seconds/3600)\n",
    "        seconds = seconds-hours*3600\n",
    "        minutes = int(seconds/60)\n",
    "        seconds = seconds - minutes*60\n",
    "        print \"Time Elapsed = %dh %dm %ds\" %(hours, minutes, seconds)\n",
    "        \n",
    "    def mean(self, l):\n",
    "        means = []\n",
    "        for i in xrange(len(l[0])):\n",
    "            s = 0\n",
    "            for j in l:\n",
    "                s += j[i]\n",
    "            means.append(float(s)/len(l))\n",
    "        return means\n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        data = pd.DataFrame(x)\n",
    "        for i in data:\n",
    "            data[i] = data[i] - data[i].mean()\n",
    "#             data[i] = data[i]/float(data[i].std())\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def feature_transform(self, x):\n",
    "#         x = sklearn.preprocessing.scale(x)\n",
    "#         scale = sklearn.preprocessing.MinMaxScaler()\n",
    "        return x #np.column_stack((x, np.exp(x), np.sqrt(scale.fit_transform(x))))\n",
    "        \n",
    "    \n",
    "    def train_stacker(self):\n",
    "        for model in self.algorithms:\n",
    "            generate_train, generate_test = model()\n",
    "            if self.testing == False:\n",
    "                self.stacking_train = np.column_stack((self.stacking_train, generate_train))\n",
    "                self.stacking_test = np.column_stack((self.stacking_test, generate_test))\n",
    "            else:\n",
    "                self.test_stacking_train = np.column_stack((self.test_stacking_train, generate_train))\n",
    "                self.test_stacking_test = np.column_stack((self.test_stacking_test, generate_test))\n",
    "        return\n",
    "        \n",
    "    def test_stacker(self):\n",
    "        if self.testing == False:\n",
    "            return [model(self.stacking_train, self.y_train2, self.stacking_test, self.y_test) \\\n",
    "                for model in self.algorithms]\n",
    "        else:\n",
    "            return [model(self.test_stacking_train, self.y2, self.test_stacking_test, None)\\\n",
    "                for model in self.algorithms]\n",
    "            \n",
    "    def initialize_data(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        if self.testing == False:\n",
    "            if self.stacking == False:\n",
    "                return self.x_train, self.y_train, self.x_test, self.y_test\n",
    "            else:\n",
    "                if x_train == None:\n",
    "                    x_train = self.x_train1\n",
    "                if y_train == None:\n",
    "                    y_train = self.y_train1\n",
    "                if x_test == None:\n",
    "                    x_test = self.x_train2\n",
    "                if y_test == None:\n",
    "                    y_test = self.y_train2\n",
    "                return x_train, y_train, x_test, y_test\n",
    "        else:\n",
    "            if self.stacking == False:\n",
    "                return self.x, self.y, self.test, None\n",
    "            else:\n",
    "                if x_train == None:\n",
    "                    x_train = self.x1\n",
    "                if y_train == None:\n",
    "                    y_train = self.y1\n",
    "                if x_test == None:\n",
    "                    x_test = self.x2\n",
    "                if y_test == None:\n",
    "                    y_test = self.y2\n",
    "                return x_train, y_train, x_test, y_test\n",
    "    \n",
    "    def stacking_test_data(self):\n",
    "        if self.testing == False:\n",
    "            return self.x_test\n",
    "        else:\n",
    "            return self.test\n",
    "        \n",
    "    def _linear_regression(self, x_train, y_train, x_test):\n",
    "        lr = linear_model.LinearRegression()\n",
    "        lr = lr.fit(x_train, y_train)\n",
    "        return lr.predict(x_test)\n",
    "    \n",
    "    def linear_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        linear_predictions = self._linear_regression(x_train, y_train, x_test)\n",
    "        if self.stacking == True:\n",
    "            return linear_predictions, self._linear_regression(x_train, y_train, self.stacking_test_data())\n",
    "        return linear_predictions\n",
    "    \n",
    "    def _ridge_regression(self, x_train, y_train, x_test):\n",
    "        lr = linear_model.Ridge()\n",
    "        lr = lr.fit(x_train, y_train)\n",
    "        return lr.predict(x_test)\n",
    "    \n",
    "    def ridge_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        linear_predictions = self._ridge_regression(x_train, y_train, x_test)\n",
    "        if self.stacking == True:\n",
    "            return linear_predictions, self._ridge_regression(x_train, y_train, self.stacking_test_data())\n",
    "        return linear_predictions\n",
    "    \n",
    "    def _lasso_regression(self, x_train, y_train, x_test):\n",
    "        lr = linear_model.Lasso()\n",
    "        lr = lr.fit(x_train, y_train)\n",
    "        return lr.predict(x_test)\n",
    "    \n",
    "    def lasso_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        linear_predictions = self._lasso_regression(x_train, y_train, x_test)\n",
    "        self.linear_preds = linear_predictions\n",
    "        if self.stacking == True:\n",
    "            return linear_predictions, self._lasso_regression(x_train, y_train, self.stacking_test_data())\n",
    "        return linear_predictions\n",
    "        \n",
    "    def xgboost(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "        if self.task == 'classification':\n",
    "            param = {'bst:max_depth':10, 'bst:eta':0.5, 'silent':0,'objective':'multi:softmax'}\n",
    "#         elif self.task == 'regression':\n",
    "#             param = {'bst:max_depth':50, 'bst:eta':1, 'silent':1,'objective':'reg:linear'}\n",
    "        param['num_class'] = x_train.shape[1]\n",
    "#         evallist = [(dtest,'eval'), (dtrain,'train')]\n",
    "        num_round = 10\n",
    "        bst = xgb.train(param, dtrain, num_round, verbose_eval=False)\n",
    "        preds = bst.predict(dtest)\n",
    "        self.xgb_predictions = preds\n",
    "#         for i in xrange(len(labels)):\n",
    "#             print labels[i], self.y_test[i]\n",
    "        if self.stacking == True:\n",
    "            return preds, bst.predict(xgb.DMatrix(self.stacking_test_data()))\n",
    "        return preds\n",
    "    \n",
    "    def gb(self, x_train=None, y_train=None, x_test=None, n_estimators=600, max_depth=2):\n",
    "        if self.task == 'classification':\n",
    "            booster = ensemble.GradientBoostingClassifier(n_estimators = n_estimators, max_depth = max_depth)\n",
    "        elif self.task == 'regression':\n",
    "            booster = ensemble.GradientBoostingRegressor(n_estimators = n_estimators, max_depth = max_depth)  \n",
    "        booster = booster.fit(x_train, y_train)\n",
    "        return booster.predict(x_test)\n",
    "    \n",
    "    def gradient_booster(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            booster_predictions = self.gb(x_train, y_train, x_test)\n",
    "            if self.stacking == True:\n",
    "                return booster_predictions, self.gb(x_train, y_train, self.stacking_test_data())\n",
    "            return booster_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.gb_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1300, 300):\n",
    "                    booster_predictions = self.gb(x_train, y_train, x_test, n_estimators=n_estimators)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        self.gb_n_estimators = n_estimators\n",
    "            if self.gb_max_depth == None:\n",
    "                max_accuracy = -1\n",
    "                for max_depth in xrange(2,15,3):\n",
    "                    booster_predictions = self.gb(x_train, y_train, x_test, max_depth = max_depth)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        self.gb_max_depth = max_depth\n",
    "                print \"Optimal Booster Hyperparams: n_estimators = %d, max_depth = %d\" \\\n",
    "                        %(self.gb_n_estimators, self.gb_max_depth)\n",
    "            booster_predictions = self.gb(x_train, y_train, x_test, n_estimators=self.gb_n_estimators,\\\n",
    "                                          max_depth=self.gb_max_depth)\n",
    "            if self.stacking == True:\n",
    "                return booster_predictions, self.gb(x_train, y_train, self.stacking_test_data())\n",
    "            return booster_predictions\n",
    "        elif self.hyperparameters == 'grid':\n",
    "            max_accuracy = -1\n",
    "            for n_estimators in xrange(100, 1000, 300):\n",
    "                for max_depth in xrange(2,15,3):\n",
    "                    booster_predictions = self.gb(x_train, y_train, x_test, n_estimators=n_estimators, max_depth = max_depth)\n",
    "                    print str(self.accuracy(booster_predictions)) + ' ' + str(n_estimators) + ' ' + str(max_depth)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        max_predictions = booster_predictions\n",
    "                        optimal_parameters = [n_estimators, max_depth]\n",
    "            print(\"Gradient booster: \", optimal_parameters)\n",
    "            return max_predictions\n",
    "                \n",
    "    def rf(self, x_train, y_train, x_test, n_estimators=1000, max_features=\"auto\"):\n",
    "        if self.task == 'classification':\n",
    "            forest = ensemble.RandomForestClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "        elif self.task == 'regression':\n",
    "            forest = ensemble.RandomForestRegressor(n_estimators=n_estimators, max_features=max_features)\n",
    "        forest = forest.fit(x_train, y_train)\n",
    "        return forest.predict(x_test)            \n",
    "        \n",
    "    \n",
    "    def random_forest(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            if self.stacking == True:\n",
    "                return self.rf(x_train, y_train, x_test), self.rf(x_train, y_train, self.stacking_test_data())\n",
    "            return self.rf(x_train, y_train, x_test)\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.rf_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1400, 400):\n",
    "                    forest_predictions = self.rf(x_train, y_train, x_test, n_estimators=n_estimators)\n",
    "                    if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(forest_predictions)\n",
    "                        self.rf_n_estimators = n_estimators\n",
    "            if self.rf_max_features == None:\n",
    "                feat = x_train.shape[1]\n",
    "                max_accuracy = -1\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.05*feat)+1):\n",
    "                    print max_features, feat\n",
    "                    if max_features == 0:\n",
    "                        max_features = 1\n",
    "                    forest_predictions = self.rf(x_train, y_train, x_test, max_features=max_features)\n",
    "                    if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(forest_predictions)\n",
    "                        self.rf_max_features = max_features\n",
    "                print \"Optimal Forest Hyperparams: n_estimators = %d, max_features = %d\" \\\n",
    "                        %(self.rf_n_estimators, self.rf_max_features)\n",
    "            forest_predictions = self.rf(x_train, y_train, x_test,\\\n",
    "                 n_estimators=self.rf_n_estimators, max_features = self.rf_max_features)\n",
    "            if self.stacking == True:\n",
    "                return forest_predictions, self.rf(x_train, y_train, self.stacking_test_data(),\\\n",
    "                                                   self.rf_n_estimators, self.rf_max_features)\n",
    "            return forest_predictions \n",
    "        elif self.hyperparameters == 'grid':\n",
    "            feat = x_train.shape[1]\n",
    "            max_accuracy = -1\n",
    "            for n_estimators in xrange(100, 1000, 200):\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.03*feat)+1):\n",
    "                    if max_features == 0:\n",
    "                        max_features = 1\n",
    "                    for criterion in ['gini', 'entropy']:\n",
    "                        forest_predictions = self.rf(x_train, y_train, x_test, n_estimators = \\\n",
    "                                 n_estimators, criterion = criterion, max_features = max_features)\n",
    "                        print self.accuracy(forest_predictions)\n",
    "                        if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                            max_accuracy = self.accuracy(forest_predictions)\n",
    "                            max_predictions = forest_predictions\n",
    "                            optimal_parameters = [n_estimators, max_features, criterion]\n",
    "            print[\"Random forest: \", optimal_parameters]\n",
    "            return max_predictions\n",
    "\n",
    "    def erf(self, x_train, y_train, x_test, n_estimators=1000, max_features=\"auto\"):\n",
    "        if self.task == 'classification':\n",
    "            forest = ensemble.ExtraTreesClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "        elif self.task == 'regression':\n",
    "            forest = ensemble.ExtraTreesRegressor(n_estimators=n_estimators, max_features=max_features)\n",
    "        forest = forest.fit(x_train, y_train)\n",
    "        return forest.predict(x_test)         \n",
    "    \n",
    "    def extremely_random_forest(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            if self.stacking == True:\n",
    "                return self.erf(x_train, y_train, x_test), self.erf(x_train, y_train, self.stacking_test_data())\n",
    "            return self.erf(x_train, y_train, x_test)\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.erf_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1300, 400):\n",
    "                    erf_predictions = self.erf(x_train, y_train, x_test, n_estimators = n_estimators)\n",
    "                    if self.accuracy(erf_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(erf_predictions)\n",
    "                        self.erf_n_estimators = n_estimators\n",
    "            if self.erf_max_features == None:\n",
    "                feat = x_train.shape[1]\n",
    "                max_accuracy = -1\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.05*feat)+1):\n",
    "                    erf_predictions = self.erf(x_train, y_train, x_test, max_features=max_features)\n",
    "                    if self.accuracy(erf_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(erf_predictions)\n",
    "                        self.erf_max_features = max_features\n",
    "                print \"Optimal erf Hyperparams: n_estimators = %d, max_features = %d\" \\\n",
    "                        %(self.erf_n_estimators, self.erf_max_features)\n",
    "            erf_predictions = self.erf(x_train, y_train, x_test,\\\n",
    "                 n_estimators=self.erf_n_estimators, max_features = self.erf_max_features)\n",
    "            if self.stacking == True:\n",
    "                return erf_predictions, self.erf(x_train, y_train, self.stacking_test_data(),\\\n",
    "                         n_estimators=self.erf_n_estimators, max_features = self.erf_max_features)\n",
    "            return erf_predictions\n",
    "    \n",
    "    def logistic_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            logistic = linear_model.LogisticRegression(penalty=\"l2\", C=1)#class_weight='auto'\n",
    "            logistic.fit(x_train, y_train)\n",
    "            logistic_predictions = logistic.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return logistic_predictions, logistic.predict(self.stacking_test_data())\n",
    "            return logistic_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.log_reg_penalty == None:\n",
    "                for penalty in ['l1', 'l2']:\n",
    "                    if penalty == 'l1':\n",
    "                        logistic = linear_model.LogisticRegression(penalty=\"l1\")#class_weight='auto'\n",
    "                        logistic.fit(x_train, y_train)\n",
    "                        logistic_predictions = logistic.predict(x_test)\n",
    "                        l1_accuracy = self.accuracy(logistic_predictions)\n",
    "                        self.log_reg_penalty = 'l1'\n",
    "                    else:\n",
    "                        logistic = linear_model.LogisticRegression(penalty=\"l2\")#class_weight='auto'\n",
    "                        logistic.fit(x_train, y_train)\n",
    "                        logistic_predictions = logistic.predict(x_test)\n",
    "                        if self.accuracy(logistic_predictions) > l1_accuracy:\n",
    "                            self.log_reg_penalty = 'l2'\n",
    "            if self.log_reg_C == None:\n",
    "                max_accuracy = -1\n",
    "                for C in xrange(1,104,4):\n",
    "                    logistic = logistic = linear_model.LogisticRegression(penalty=self.log_reg_penalty, C=C)\n",
    "                    logistic.fit(x_train, y_train)\n",
    "                    logistic_predictions = logistic.predict(x_test)\n",
    "                    if self.accuracy(logistic_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(logistic_predictions)\n",
    "                        self.log_reg_C = C\n",
    "                print \"Optimal Logistic Regression Hyperparams: Penalty = %s, C = %d.\"\\\n",
    "                    %(self.log_reg_penalty, self.log_reg_C)\n",
    "            logistic = linear_model.LogisticRegression(penalty=self.log_reg_penalty, C = self.log_reg_C)#class_weight='auto'\n",
    "            logistic.fit(x_train, y_train)\n",
    "            logistic_predictions = logistic.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return logistic_predictions, logistic.predict(self.stacking_test_data())\n",
    "            return logistic_predictions\n",
    "    \n",
    "    def lda(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        lda = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "        lda.fit(x_train, y_train)\n",
    "        lda_predictions = lda.predict(x_test)\n",
    "        if self.stacking == True:\n",
    "            return lda_predictions, lda.predict(self.stacking_test_data())\n",
    "        return lda_predictions\n",
    "    \n",
    "    def qda(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            qda = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "            qda.fit(x_train, y_train)\n",
    "            qda_predictions = qda.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return qda_predictions, qda.predict(self.stacking_test_data())            \n",
    "            return qda_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.qda_reg_param == None:\n",
    "                max_accuracy = -1\n",
    "                for reg_param in [x*.1 for x in range(0,10,2)]:\n",
    "                    qda = discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param = reg_param)\n",
    "                    qda.fit(x_train, y_train)\n",
    "                    qda_predictions = qda.predict(x_test)\n",
    "                    if self.accuracy(qda_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(qda_predictions)\n",
    "                        self.qda_reg_param = reg_param\n",
    "                print \"Optimal QDA Hyperparams: Reg_param = %d\" %(self.qda_reg_param)\n",
    "            qda = discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param = self.qda_reg_param)\n",
    "            qda.fit(x_train, y_train)\n",
    "            qda_predictions = qda.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return qda_predictions, qda.predict(self.stacking_test_data())\n",
    "            return qda_predictions\n",
    "    \n",
    "    def svc(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            support_vm = svm.SVC()\n",
    "            support_vm.fit(x_train, y_train)\n",
    "            svm_predictions = support_vm.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return svm_predictions, s.predict(self.stacking_test_data())\n",
    "            return svm_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.svc_C == None:\n",
    "                max_accuracy = -1\n",
    "                for C in [10, 100]:\n",
    "                    s = svm.SVC(C=C)\n",
    "                    s.fit(x_train, y_train)\n",
    "                    svm_predictions = s.predict(x_test)\n",
    "                    if self.accuracy(svm_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(svm_predictions)\n",
    "                        self.svc_C = C\n",
    "            if self.svc_kernel == None:\n",
    "                max_accuracy = -1\n",
    "                for kernel in ['rbf', 'linear', 'poly']:\n",
    "                    s = svm.SVC(C = self.svc_C, kernel=kernel)\n",
    "                    s.fit(x_train, y_train)\n",
    "                    svm_predictions = s.predict(x_test)\n",
    "                    if self.accuracy(svm_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(svm_predictions)\n",
    "                        self.svc_kernel = kernel\n",
    "                print \"Optimal SVM Hyperparams: Kernel = %s, C = %d\" %(self.svc_kernel, self.svc_C)\n",
    "            s = svm.SVC(C = self.svc_C, kernel=self.svc_kernel)\n",
    "            s.fit(x_train, y_train)\n",
    "            svm_predictions = s.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return svm_predictions, s.predict(self.stacking_test_data())\n",
    "            return svm_predictions\n",
    "    \n",
    "    def blend(self, args):\n",
    "        blended_predictions = []\n",
    "        if self.task == 'classification':\n",
    "            n = len(args)\n",
    "            for i in xrange(len(args[0])):\n",
    "                counts = {}\n",
    "                for j in xrange(n):\n",
    "                    try:\n",
    "                        counts[args[j][i]] += 1\n",
    "                    except KeyError:\n",
    "                        counts[args[j][i]] = 1\n",
    "                maximus = -float('inf')\n",
    "                for index, value in counts.iteritems():\n",
    "                    if value > maximus:\n",
    "                        max_value = index\n",
    "                        maximus = value\n",
    "                blended_predictions.append(max_value)\n",
    "        elif self.task == 'regression':\n",
    "            blended_predictions = self.mean(args)\n",
    "        return blended_predictions\n",
    "    \n",
    "    def run_algorithms(self):\n",
    "        if self.task == 'classification':\n",
    "            return [i() for i in self.algorithms]\n",
    "        elif self.task == 'regression':\n",
    "            return [i() for i in self.algorithms]\n",
    "    \n",
    "    def create_predictions(self):\n",
    "        self.testing = True\n",
    "        predictions = (self.run_algorithms() + self.stacker())\n",
    "        self.testing = False\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions).transpose()\n",
    "        self.testing_df = predictions_df\n",
    "        predictions_df.columns = self.algorithm_names\n",
    "        \n",
    "        optimal_predictions = self.blend([predictions_df[j].values for j in self.optimal_iterator])\n",
    "        self.time_elapsed()\n",
    "        return optimal_predictions\n",
    "    \n",
    "    def predict(self):\n",
    "        algorithms = self.run_algorithms()\n",
    "\n",
    "        for i, v in enumerate(algorithms):\n",
    "            print self.accuracy(v), self.algorithm_names[i]\n",
    "        \n",
    "        predictions = (algorithms + self.stacker())\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions).transpose()\n",
    "        self.training_df = predictions_df\n",
    "        predictions_df.columns = self.algorithm_names\n",
    "\n",
    "        combinations = []     \n",
    "        if self.task == 'classification':\n",
    "            col = predictions_df.columns\n",
    "            for i in xrange(1, len(col), 2):\n",
    "                for j in itertools.combinations(col, i):\n",
    "                    combinations.append([k for k in j])            \n",
    "            maximus = -1\n",
    "            for i in combinations:\n",
    "                blended = self.blend([predictions_df[j].values for j in i])\n",
    "                accuracy = self.accuracy(blended)\n",
    "                if accuracy > maximus:\n",
    "                    maximus = accuracy\n",
    "                    optimal_predictions = blended\n",
    "                    self.optimal_iterator = i\n",
    "                    print str(accuracy) + str(i)            \n",
    "        elif self.task == 'regression':\n",
    "            col = predictions_df.columns\n",
    "            for i in xrange(1, len(col)):\n",
    "                for j in itertools.combinations(col, i):\n",
    "                    combinations.append([k for k in j])            \n",
    "            maximus = float(\"inf\")\n",
    "            for i in combinations:\n",
    "                blended = self.blend([predictions_df[j].values for j in i])\n",
    "                accuracy = self.accuracy(blended)\n",
    "                if accuracy < maximus:\n",
    "                    maximus = accuracy\n",
    "                    optimal_predictions = blended\n",
    "                    self.optimal_iterator = i\n",
    "                    print str(accuracy) + str(i)\n",
    "        \n",
    "        if self.test != None: \n",
    "            return self.create_predictions()\n",
    "        else:\n",
    "            self.time_elapsed()\n",
    "            return optimal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.127371331579 linear_predictions\n",
      "0.12351153089 ridge_predictions\n",
      "0.688312085527 lasso_predictions\n",
      "0.171884002777 booster_predictions\n",
      "0.105875 forest_predictions\n",
      "0.0989375 erf_predictions\n",
      "0.127371331579['linear_predictions']\n",
      "0.12351153089['ridge_predictions']\n",
      "0.105875['forest_predictions']\n",
      "0.0989375['erf_predictions']\n",
      "0.0948125['erf_on_stack']\n",
      "0.090625['forest_predictions', 'erf_predictions']\n",
      "0.090480952381['forest_predictions', 'erf_predictions', 'erf_on_stack']\n",
      "Time Elapsed = 0h 0m 5s\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(path + \"iris.data\", header=None)\n",
    "y = iris[3]\n",
    "iris = iris.drop([3], 1)\n",
    "iris = iris.drop([4], 1)\n",
    "problem = apm(iris, y, task='regression', hyperparameters='default')\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malware:\n",
      "0.803643724696 xgboost_predictions\n",
      "0.864372469636 booster_predictions\n",
      "0.864372469636 forest_predictions\n",
      "0.854251012146 erf_predictions\n",
      "0.84008097166 logistic_predictions\n",
      "0.726720647773 lda_predictions\n",
      "0.564777327935 qda_predictions\n",
      "0.803643724696['xgboost_predictions']\n",
      "0.864372469636['booster_predictions']\n",
      "0.868421052632['forest_on_stack']\n",
      "0.872469635628['xgboost_predictions', 'booster_predictions', 'forest_predictions']\n",
      "0.874493927126['xgboost_predictions', 'booster_predictions', 'forest_on_stack']\n",
      "0.876518218623['booster_predictions', 'logistic_predictions', 'forest_on_stack']\n",
      "0.878542510121['xgboost_predictions', 'booster_predictions', 'forest_predictions', 'logistic_predictions', 'booster_on_stack']\n",
      "0.882591093117['xgboost_predictions', 'booster_predictions', 'forest_predictions', 'qda_predictions', 'booster_on_stack']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremynixon/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:612: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed = 1h 1m 56s\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"/Users/jeremynixon/Dropbox/Kiko/w2v+sequence.csv\", header=None)\n",
    "labels = pd.read_csv(\"/Users/jeremynixon/Dropbox/Kiko/labels.csv\", header=None)\n",
    "x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(train, labels, test_size = .20, random_state=42)\n",
    "print \"Malware:\"\n",
    "problem = apm(x_train, np.array(y_train.T)[0], task='classification', test=x_test, hyperparameters='default', debug=True)\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540625 xgboost_predictions\n",
      "0.653125 booster_predictions\n",
      "0.646875 forest_predictions\n",
      "0.665625 erf_predictions\n",
      "0.55625 logistic_predictions\n",
      "0.546875 lda_predictions\n",
      "0.565625 qda_predictions\n",
      "0.540625['xgboost_predictions']\n",
      "0.653125['booster_predictions']\n",
      "0.665625['erf_predictions']\n",
      "0.66875['xgboost_predictions', 'erf_predictions', 'lda_predictions']\n",
      "0.671875['booster_predictions', 'erf_predictions', 'forest_on_stack']\n",
      "0.675['xgboost_predictions', 'booster_predictions', 'forest_predictions', 'erf_predictions', 'lda_predictions']\n",
      "0.6875['xgboost_predictions', 'booster_predictions', 'forest_predictions', 'erf_predictions', 'qda_predictions']\n",
      "0.690625['xgboost_predictions', 'booster_predictions', 'erf_predictions', 'lda_predictions', 'xgboost_on_stack', 'booster_on_stack', 'forest_on_stack']\n",
      "Time Elapsed = 0h 0m 32s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "Y = df['quality'].values\n",
    "le = preprocessing.LabelEncoder().fit(Y)\n",
    "Y = le.transform(Y)\n",
    "df = preprocessing.scale(df.drop('quality',1))\n",
    "problem = apm(df, Y, task='classification', hyperparameters='default')\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
